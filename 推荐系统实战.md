# 推荐系统实战

## 第一章 好的推荐系统

* 2011年的Recsys大会专门邀请了Pandora的研究人员对音乐推荐进行了演讲
* 《长尾理论》
* [AB测试](http://www.abtests.com/)
* 信任度--评论网站Epinion

### 评测推荐系统的方法

* 离线实验
* 在线实验
* 用户调查

*********************

## 第二章 利用用户的行为数据

* 学术界提出了三种协同过滤方法
 * 基于领域的方法
 * 隐语义模型
 * 基于图的随机游走算法
* 应用最广的算法为基于领域的方法
 * 基于用户的协同过滤方法
   给用户推荐和他兴趣相似的其他用户喜欢的物品
 * 基于物品的协同过滤方法
   给用户推荐和他喜欢的物品相类似的物品

* [MovieLens数据集](http://www.grouplens.org/node/73)

### 2.4 基于领域的算法

#### 2.4.1 基于用户的过滤算法

* 找到和目标用户兴趣相似的用户集合
* 找到这个集合中的用户喜欢，且目标用户没有听说过的物品推荐给目标用户

* 缺点：随着用户数目越来越大，计算用户兴趣相似度矩阵越来越困难，其运算时间复杂度和空间复杂度和用户增长近似于平方关系，其次，基于用户的协同过滤很难对推荐结果做出解释。
  
##### 计算用户的兴趣相似度

给定用户u,v, 令N(u)表示用户u曾经有过正反馈的物品集合，令N(v)表示用户v曾经有过的正反馈物品集合

* 通过Jaccard公式计算u和v的兴趣相似度

$$w^j_{uv} = \frac{\vert{N(u)\cap{N(v)}\vert}}{\vert{N(u)\cup{N(v)}}\vert}$$

* 通过余弦相似度计算

$$w^c_{uv} = \frac{\vert{N(u)\cap{N(v)}\vert}}{\vert{N(u)\vert\vert{N(v)}}\vert}$$

* e.g.

用户A对物品{a, b, d}有过行为，用户B对物品{a, c}有过行为

$$w^j_{uv} = \frac{\vert{\{a, b, d\}\cap{\{a, c\}}\vert}}{\vert{\{a, b, d\}\cup{\{a, c\}}}\vert} = \frac{1}{4}$$

$$w^c_{uv} = \frac{\vert{\{a, b, d\}\cap{\{a, c\}}\vert}}{\vert{\{a, b, d\}\vert\vert{\{a, c\}}}\vert} = \frac{1}{\sqrt{6}}$$

##### UserCF算法

给用户推荐和他兴趣最相似的K个用户喜欢的物品，下面是计算用户u对物品i感兴趣的程度\
简单来说就是将与用户u兴趣最相似的K个用户的相似度对物品i的兴趣的乘积求和，得到用户u对物品i的兴趣度的预测

$$p(u,i) = \sum_{v\in S(u,K)\cap N(i)}w_{uv}r_{vi}$$

$S(u,K)$: 包含用户u兴趣最接近的K个用户\
$N(i)$: 对物品i有过行为的用户集合\
$r_{uv}$: 代表用户v对物品i的兴趣

##### 用户相似度的改进

e.g.: 对于流行度不同的物品，在计算用户相似度时应提供不同的贡献。以图书为例，a,b两个用户同时购买《新华字典》, c,d两个用户同时购买《白夜行》，我们可以说c，d用户兴趣相似，但不能说a，b两个用户兴趣相似，因为《新华字典》大多数中国人小时候都买过。

##### User-IIF算法

惩罚流行度高的物品在计算用户相似度时的影响

$$w_{uv} = \frac{\sum_{i\in N(u)\cap N(v)}\frac{1}{\log1 + \vert N(i) \vert}}{\vert{N(u)\vert\vert{N(v)}}\vert}$$


#### 2.4.2 基于物品的协同过滤算法--ItemCF

**注**: ItemCF算法并不利用物品的内容属性计算物品之间的相似度，它主要通过分析用户的行为记录计算物品之间的相似度。

e.g.:该算法认为，物品A和物品B具有很大的相似度是因为喜欢物品A的用户大都也喜欢物品B

* 计算物品之间的 相似度
* 根据物品的相似度和用户的历史行为给用户生成推荐列表

$$w_{ij} = \frac{\vert N(i)\cap N(j)\vert}{\vert N(i)\vert}$$

$N(i)$: 喜欢物品i的用户
$\vert N(i)\vert$: 喜欢物品i的用户数

上述公式描述了喜欢物品i的用户中有多少同时喜欢物品j。但这种计算方法会带来一个问题，如果物品i是热门物品，很多人喜欢，那么$w_{ij}$就会很大，极端情况就是所有喜欢物品j的用户都喜欢物品i，$w_{ij}==1$.这会造成该算法推荐的物品和热门物品有很大的相似度。

为了避免这个问题，可以用下面的公式：

$$w_{ij} = \frac{\vert N(i)\cap N(j)\vert}{\sqrt{\vert N(i)\vert \vert N(j)\vert}}$$

进阶版
$$w_{ij} = \frac{\vert N(i)\cap N(j)\vert}{\vert N(i)\vert ^{1-\alpha} \vert N(j)\vert ^{\alpha}}$$

##### ItemCF算法

$$p(u,j) = \sum_{i\in S(j,K)\cap N(u)}w_{ji}r_{ui}$$

$S(j,K)$: 和物品j最相似的K个物品的几何
$r_{ui}$: 用户u对物品i的兴趣度(对于隐反馈数据集，如果用户u对物品i有过行为，即可零$r_{ui}=1$)


##### ItemCF-IUF算法

我们假设这样一个情况，一个书店老板为了开书店去当当网买了80万本书。根据ItemCF算法，这意味着80万本书两两之间产生了相似度，但是这各用户并不是出于自身的兴趣购买的图书。

所以在计算物品相似度时，应该根据用户的活跃度给予不同的权重，使得活跃度越高的用户贡献度越小。

$$w_{ij} = \frac{\sum_{i\in N(i)\cap N(j)}\frac{1}{\log1 + \vert N(u) \vert}}{\sqrt{\vert{N(i)\vert\vert{N(j)}}\vert}}$$

对于过于活跃的用户我们直接忽略他的兴趣列表。

##### 物品相似度归一化

Karypis在研究中心发现如果将ItemCF的相似度矩阵按最大值归一化，可以提高推荐的准确率。(Evaluation of Item-based Top-N Recommendation Algorithms)

$$w^{'}_{ij} = \frac{w_{ij}}{\max_{j} w_{ij}}$$


**总结**

* UserCF算法给用户推荐那些和他有共同兴趣爱好的用户喜欢的物品。UserCF的推荐更加社会化，反映了用户所在的小型兴趣群体中物品的热门程度。
* ItemCF给用户推荐那些和他之前喜欢的物品类似的物品。ItemCF推荐更加个性化，反映了用户自己的兴趣传承。


### 2.5 隐语义模型-LFM(laten factor model)

LFM通过如下公司计算用户u对物品i的兴趣

$$Preference(u,i) = r_{ui} = p^{T}_{u}q_{i} = \sum^F_{f=1}p_{u,k}q_{i,k}$$

$p_{u,k}$: 用户u的兴趣和第k个隐类的关系
$q_{i,k}$: 第k个隐类和物品i之间的关系

因为隐性反馈数据集只有正样本(用户喜欢什么物品)，而没有负样本(用户对什么物品不感兴趣)，所以，我们需要给每个用户生成负样本

生成负样本应遵循以下原则

* 对于每个用户，要保证正负样本的平衡(数目相似)
* 对每个用户采样负样本时，要选取那些很热门的，而用户却没有行为的物品

解释：一般认为，很热门而用户却没有行为更加代表用户对这个物品不感兴趣。因为对于冷门的物品，用户可能是压根没在网站中发现这个物品，所以谈不上是否感兴趣

我们通过优化如下损失函数来找到最合适的参数p和q

$$C=\sum_{(u,i)\in K}(r_{ui} - \hat r_{ui})^2 = \sum_{(u,i)\in K}(r_{ui} - \sum^K_{k=1}p_{u,k}q_{i,k})^2 + \lambda \Vert p_u\Vert ^2 + \lambda \Vert q_i\Vert ^2$$

使用随机梯度方法更新参数$p_{uk}$ $q_{ik}$

$$\frac{\partial C}{\partial p_{uk}} = -2q_{ik} + 2\lambda p_{uk}\quad
\frac{\partial C}{\partial q_{ik}} = -2p_{uk} + 2\lambda q_{ik}$$

$$p_{uk} = p_{uk} + \alpha(q_{ik} - \lambda p_{uk})
\quad
q_{ik} = q_{ik} + \alpha(p_{uk} - \lambda q_{ik})$$

**在LFM中，重要的参数有4个**
* 隐特征的个数F
* 学习速率$\alpha$
* 正则化参数$\lambda$
* 负样本/正样本比例ratio

### 2.6 基于图的模型

****************

## 第三章 推荐系统冷启动问题

* **冷启动问题**
如何在没有大量用户数据的情况下设计个性化推荐新系统并且让用户对推荐结果满意从而愿意使用推荐系统

* **冷启动问题主要分为三类**
  * 用户冷启动: 如何给新用户做个性化推荐
  * 物品冷启动: 如何将新物品推荐给可能对它感兴趣的用户
  * 系统冷启动: 如何在一个新开发的网站上设计推荐系统

### 用户冷启动方案

#### 提供非个性化推荐

针对新用户，我们可以给用户推荐热门排行榜，然后等到用户数据收集到一定的时候，再切换为个性化推荐。

#### 利用用户注册信息

基于注册信息的个性化推荐流程如下:
* 获取用户注册信息
* 根据用户的注册信息对用户分类
* 给用户推荐他所属分类中用户喜欢的物品

用户注册信息主要分3类
* 人口统计学特征: 包括用户的年龄、性别、职业、民族、学历和居住地
* 用户兴趣的描述：有一些网站会让用户用文字描述他们的兴趣
* 从其他网站导入的用户站外行为数据

基于用户注册信息的推荐算法其核心问题是计算具有特征$f$的用户对于物品$i$的喜好程度$p(f,i)$

基本算法
$$p(f,i) = \vert N(i)\cap U(f)\vert$$

$N(i)$: 喜好物品$i$的用户
$U(f)$: 具有特征$f$的用户

问题：这种算法会导致热门物品会在各种用户中都具有比较高的权重

改进算法

$$p(f,i) = \frac{\vert N(i)\cap U(f)\vert}{\vert N(i)\vert + \alpha}$$

$\alpha$解释：参数$\alpha$目的是解决数据稀疏的问题。假定有一个物品只被一个用户喜欢过，而这个用户刚好就有特征$f$，那么就有$p(f,i)=1$.但是这种情况并没有统计意义。因此我们为分母加上一个比较大的数，避免这样的物品产生比较大的权重。

#### 选择合适的物品启动用户的兴趣

在新用户第一次访问推荐系统时，给用户提供一些物品，让用户反馈他们对这些物品的兴趣，然后根据用户反馈给提供个性化推荐。

一般来说，能够用来启动用户兴趣的物品需要具有以下特点

* 比较热门：用户必须了解这个物品
* 具有代表性和区分性：这要求物品不能是大众化或老少咸宜的，因为这样的物品对用户的兴趣没有区分性
* 启动物品集合需要多样性：因为用户兴趣的可能性非常多，所以我们需要具有很高覆盖率的物品，这些物品能覆盖几乎所有主流的用户兴趣

问题：如何设计一个选择启动物品集合的系统

Nadav Golbandi的[论文](http://research.yahoo.com/pub/3502)提出用用户对物品评分的方差度量这群用户兴趣的一致程度。

计算物品$i$的区分度$D(i)$

$$D(i) = \sigma_{u\in N^{+}(i)} + \sigma_{u\in N^-(i)} + \sigma_{u\in \bar N(i)}$$

$N^+(i)$: 喜欢物品$i$的用户集合\
$N^-(i)$: 不喜欢物品$i$的用户集合\
$\bar N(i)$: 没有对物品$i$评分的用户集合\
$\sigma_{u\in N^{+}(i)}$: 喜欢物品$i$的用户对其他物品评分的方差\
$\sigma_{u\in N^-(i)}$: 不喜欢物品$i$的用户对其他物品评分的方差\
$\sigma_{u\in \bar N(i)}$: 没有对物品$i$评分的用户对其他物品评分的方差

### 3.4 利用物品的内容信息

物品冷启动需要解决的问题是如何将新加入的物品推荐给对它感兴趣的用户

#### 内容过滤算法--ContentItemKNN

提取物品内容信息的关键词，将其表示为向量

对于物品d，它的内容表示成一个关键词向量如下:

$$d = \{(e_1, w_1), (e_2, w_2), ...\}$$

$e_i$: 关键词
$w_i$: 关键词对应的权重

对于文本物品，可以用信息检索领域著名的TF-IDF公式计算词的权重

$$w_i = \frac{TF(e_i)}{\log DF(e_i)}$$

然后计算余弦相似度

$$w_{ij} = \frac{d_i\cdot d_j}{\sqrt{\Vert d_{i}\Vert d_j\Vert}}$$

### 3.5 发挥专家的作用
很多推荐系统在建立时，既没有用户的行为数据，也没有充足的物品内容信息来计算准确的物品相似度。那么为了在推荐系统建立时就让用户得到比较好的体验，很多系统都利用专家进行标注。

代表系统: 个性化网络电台Pandora 电影推荐网站Jinni

***************************

## 第4章 利用用户标签数据

推荐系统的目的是联系用户的兴趣和物品:
* 1：利用用户喜欢过的物品，给用户推荐与他喜欢过的物品相似的物品
* 2：利用和用户兴趣相似的其他用户，给用户推荐那些和他们兴趣爱好相似的其他用户喜欢的物品
* 3：通过一些特征联系用户和物品，给用户推荐那些具有用户喜欢的特征的物品

通过给物品打标签的形式描述物品的特征
* 1：UGC:让普通用户给物品打标签
* 2：让专家给物品打标签


#### 标签系统的主要问题

* 基于标签的推荐：如何利用用户打标签的行为为其推荐物品
* 标签 推荐：如何在用户给物品打标签时为其推荐适合物品的标签

注：基于标签的物品推荐算法——Delicious数据集、CiteULike数据集

**准确率**

$$Precision = \frac{\vert R(u)\cap T(u\vert)}{\vert R(u)\vert}$$

**召回率**

$$Recall = \frac{\vert R(u)\cap T(u)\vert}{\vert T(u)\vert}$$

**覆盖率**

$$Coverage = \frac{\vert\bigcup_{u\in U}R(u)}{I}$$

**多样性定义**

$$Diversity = 1 - \frac{\sum_{i \in R(u)} \sum_{j \in R(u),j \neq i} Sim(item\_tags[i], item\_tags[j])}{}$$

**平均热门程度度量**

$$AveragePopularity = \frac{\sum_{u}\sum_{i \in R(u)}\log(1 + item\_pop(i)}{\sum_{u}\sum_{i\in R(u)}1}$$

$item\_tags[i]$: 存储了物品$i$的标签向量
$R(u)$:用户$u$的长度为$N$的推荐列表，里面包含我们认为用户会打标签的物品
$T(u)$:是测试集中用户$u$实际上打过标签的物品集合
$item\_pop(i)$$:为这个物品打过标签的用户数

**一个简单的算法**

* 统计用户最常用的标签
* 对于每个标签，统计被打过这个标签次数最多的物品
* 对于一个用户，首先找到他常用的标签，然后找到具有这些标签的最热门物品推荐给这个用户

$$p(u,i) = \sum_b n_{u,b}n_{b,i}$$

$n_{u,b}$:用户$u$打过标签$b$的次数
$n_{b,i}$:物品$i$被打标签$b$的次数

$B(i)$:物品$i$的标签集合
$B(u)$:用户$u$的标签集合
**缺点**: 这个公式倾向于给热门标签对应的热门物品很大的权重，因此会造成推荐热门的物品给用户，从而降低推荐结果的新颖性。

#### 算法改进

##### 1.TF-IDF

**TagBasedTFIDF--降低热门标签的权重**

$$p(u,i) = \sum_{b}\frac{n_{u,b}}{\log(1+n^{(u)}_{b})}n_{b,i}$$

**TagBasedTFID++--同时降低热门物品的权重**

$$p(u,i) = \sum_{b}\frac{n_{u,b}}{\log(1+n^{(u)}_{b})}\frac{n_{b,i}}{\log(1+n^{(u)}_{i})}$$

$n^{(u)}_{b}$:使用标签$b$的用户数
$n^{(u)}_{i}$:给物品$i$打过标签的用户数

##### 2.数据稀疏

**问题**:用户兴趣和物品的联系是通过$B(u)\cap B(i)$建立的。但是，对于新用户和新物品，这个集合中的标签数量会很少。

**解决方案**:通过添加相似的标签扩展用户的标签集合(思考:为什么不扩展物品的标签集合)

**计算标签相似度**

$$sim(b,b^{'}) = \frac{\sum_{i\in N(b)\cap N(b^{'})}n_{b,i}n_{b^{'},i}}{\sqrt{\sum_{i\in N(b)}n^2_{b,i}\sum_{i\in N(b^{'})}n^2_{b^{'},i}}}$$

$N(b)$:有标签$b$的物品集合
$n_{b,i}$:给物品$i$打标签$b$的用户数

##### 3.标签清理

**问题**

* 不是所有标签都能用来做推荐，比如“不好笑”这个标签，我们不能给用户继续推荐含有“不好笑”标签的物品
* 去除词义相同的标签：如recommender system 和 recommendation engine

**标签清理方法**

* 去除词频很高的停止词
* 去除因词根不同造成的同义词，比如recommender system 和 recommendation system
* 去除因分隔符造成的同义词，比如collaborative_filtering和collaborative-filtering

为了控制标签的质量，很多网站也采用了让用户进行反馈的思想，即让用户告诉系统某个标签是否合适

#### 基于标签的推荐

将用户和物品之间的关系变成用户对标签的兴趣以及标签和物品的相关度

**解释**:无论是基于物品的推荐算法，还是基于用户的推荐算法，本质上都是直接推荐物品给用户。而基于标签的推荐算法，推荐过程分两步，首先根据用户的历史兴趣推荐相关度高的标签，然后针对推荐的标签推荐与之相关度高的物品。

**举例**:用户看了几本侦探小说，用户标签集合里有了**侦探**
，与**侦探**标签相似度高的标签有**东野圭吾**，与**东野圭吾**标签最相关的物品为**白夜行**，于是推荐系统给用户推荐**白夜行**物品。

##### 给用户推荐标签

**问题**: 为什么要给用户推荐标签

**理由**

* 方便用户输入标签
* 提高标签的质量

**如何给用户推荐标签**

* 推荐最热门的标签
* ItemPopularTags: 给用户$u$推荐物品$i$最热门的标签
* UserPopularTags: 给用户推荐他最常用的标签
* HybridPopularTags: 通过一个系数将上面的推荐结果线性加权，然后生成最终的推荐结果


## 利用上下文信息

### 时间上下文信息

#### 时间效应简介

* **用户兴趣是变化的**：比如随着年龄的增长，用户小时候喜欢看动画片，长大了喜欢看文艺片。一位程序员随着工作时间的增加，逐渐从阅读入门书籍过度到阅读专业书籍。
* **物品也是有生命周期的**：一部电影刚上映的时候可能被很多人关注，但是经久不衰的电影是很少的。当我们决定在某个时刻给某个用户推荐某个物品时，需要考虑该物品在该时刻是否已经过时了。
* **季节效应**：季节效应主要反映了时间本身对用户兴趣的影响。比如人们夏天吃冰淇淋，冬天吃火锅，夏天穿T恤，冬天穿棉衣。详细信息关乎2011年ACM推荐大会的一个上下文相关的电影推荐算法比赛。

#### 系统时间特性分析

**通过统计如下信息研究系统的时间特性**

* 数据集每天独立用户的增长情况
* 系统的物品变化情况
* 用户访问情况

**物品的生存周期和系统的时效性**

**物品平均在线天数**：如果一个物品在某天被至少一个用户产生行为，就定义该物品在这一天在线。因此我们可以通过物品的平均在线天数度量一类物品的生存周期。
**相隔T天系统物品流行度向量的平均相似度**：取系统中相邻T天的两天，分别计算这两天的物品流行度，从而得到两个流行度向量。计算这两个向量的余弦相似度，如果相似度大，说明系统的物品在相隔T天的时间内发生了很大的变化，从而说明系统的时效性很强，物品 平均在线时间很短，反之说明时效性弱，平均在线时间长。


**问题**：如何提高推荐结果的多样性
**解决方案**：
* 保证推荐系统能够在用户有了新行为后及时调整推荐结果，使推荐结果满足用户最近的兴趣
* 需要保证推荐系统在用户没有新的行为时也能够经常变化一下结果，具有一定的时间多样性
  

**问题**：如果用户没有行为，如何保证给用户的推荐结果具有一定的时间多样性
**解决方案**
* 在生成推荐结果时加入一定的随机性
* 记录用户每天看到的推荐结果，然后在每天给用户进行推荐时，对他前几天看到过很多次的推荐结果进行适当地降权
* 每天给用户使用不同的推荐算法

### 时间上下文推荐算法

$$n_{i}(T) = \sum_{(u,i,t)\in Train, t < T} \frac{1}{1 + \alpha(T-t)}$$

$n_{i}(T)$: 物品$i$最近的流行度
$(u,i,t)$: 用户$u$在$t$时刻对物品$i$有过行为

#### 时间上下文相关的ItemCF算法

**计算物品相似度**

$$sim(i,j) = \frac{\sum_{u\in N(i)\cap N(j)}1}{\sqrt{\vert N(i) \vert \vert N(j)\vert}}$$

**用户$u$对物品$i$的兴趣$p(u,i)$**

$$p(u,i) = \sum_{j\in N(u)} sim(i, j)$$

**利用时间信息改进相似度的计算**

$$sim(i,j) = \frac{\sum_{u\in N(i)\cap N(j)}f(|\vert t_{ui} - t_{uj}\vert)}{\sqrt{\vert N(i)\vert\vert N(j)\vert}}$$

$f(\vert t_{ui} - t_{uj}\vert)=\frac{1}{1 + \alpha\vert t_{ui}-t_{uj}\vert}$:时间衰减项
$\alpha$:时间衰减项，若一个系统的用户兴趣变化很快，就应该取比较大的$\alpha$,反之需要去比较小的$\alpha$
$t_{ui}$:用户$u$对物品$i$产生行为的时间

$$p(u,i) = \sum_{j\in N(u)\cap S(i,K)}sim(i,j)\frac{1}{1 + \beta\vert t_0 - t_{uj}\vert}$$

$t_0$:当前时间

#### 时间上下文相关的UserCF算法

**计算用户相似度**

$$w_{uv} = \frac{\vert N(u)\cap N(v)\vert}{\sqrt{\vert N(u)\vert\cup\vert N(v)\vert}}$$

**利用时间信息改进用户相似度的计算**

$$w_{uv} = \frac{\sum_{i\in N(u)\cap N(v)}\frac{1}{1+\alpha\vert t_{ui}-t_{vi}}}{\sqrt{\vert N(u)\vert\cup\vert N(v)\vert}}$$

**计算用户对物品的兴趣**

$$p(u,i) = \sum_{v\in S(u,K)}w_{uv}r_{vi}\frac{1}{1 + \alpha(t_{0} - t_{vi})}$$


### 基于位置的推荐算法

**LARS算法**

$$RecScore(u,i) = P(u,i) - TravelPenalty(u,i)$$

$TraveIPenalty$:表示物品$i$的位置对用户$u$的代价，一般会结合交通数据计算物品$i$与用户$u$之前评分的所有物品的位置计算距离的平均值(或者最小值)

为了避免计算用户对所有物品的$TraverlPenalty$, $LARS$在计算用户$u$对物品$i$的兴趣度$ReScore(u,i)$时，首先对用户每一个曾经评过分得物品(一般是参观、商店、景点)，找到和他距离小于一个阈值的所有其他物品，然后将这些物品的集合作为候选集，然后利用上面的公式计算最终的$ReScore$

# 推荐算法

目前实用化的工业界的推荐系统通常由两个环节构成，**召回阶段**，**排序阶段**

第一阶段：召回阶段
常用做法：多路召回，每一路召回采取一个不同的策略

## 协同过滤算法

## LFM

* **基本模型**

$$p(u,i) = r_{ui} = p^{T}_{u}q_{i} = \sum^F_{f=1}p_{u,f}q_{i,f}$$
计算用户$u$对物品$i$的兴趣，也可计算用户$u$对物品$i$的点击率

* **参数解释**

$p(u,i)$:用户$u$对物品$i$的兴趣度(或者点击概率,根据业务需要有不同的解释)\
$F$:代表隐变量的个数\
$p_u$:代表用户向量，用$F$个隐变量表示\
$q_i$:代表物品向量，用$F$个隐变量表示

* **实例解释**
![](\img\LFM.png)
如图所示，我们通过LFM算法得出了$p_{u=1}$和$q_{i=1}$的隐变量值，这样我们就可以通过$\sum^F_{f=1}p_{u=1,f}q_{i=1,f}$预测用户$user\_1$对于物品$item\_1$的兴趣度

* **模型求解**

损失函数

$$Loss=\sum_{(u,i)\in K}(r_{ui} - \hat r_{ui})^2 = \sum_{(u,i)\in K}(r_{ui} - \sum^K_{k=1}p_{u,k}q_{i,k})^2 + \lambda \Vert p_u\Vert ^2 + \lambda \Vert q_i\Vert ^2$$

使用SGD优化参数

$$\frac{\partial C}{\partial p_{uk}} = -2q_{ik} + 2\lambda p_{uk}\quad
\frac{\partial C}{\partial q_{ik}} = -2p_{uk} + 2\lambda q_{ik}$$

$$p_{uk} = p_{uk} + \alpha(q_{ik} - \lambda p_{uk})
\quad
q_{ik} = q_{ik} + \alpha(p_{uk} - \lambda q_{ik})$$

***********************

## FM系列算法

### FM(Factorization Machines<因子分解机>)

* **优点**
  * 在高度稀疏的情况下特征之间的交叉仍然能够被估计
  * 参数的学习和模型的预测的时间都是线性的

* **基本模型**

$$ \hat{y} = w_0 + \sum^{n}_{i=1}{w_{i}x_{i}} + \sum^{n}_{i = 1}\sum^n_{j=i+1}\langle v_i, v_j\rangle x_i x_j$$

$w_i和v_i$为待求参数(先随机取值，然后通过SGD优化，类似$NN$中的参数)
$\langle v_i, v_j\rangle$:表示特征$x_i$和特征$x_j$组合特征的权重
$x_i.shape = (n, m)$表示样本集有$n$个参数，$m$个样本

* **问题1** <u>为什么$FM$模型解决了数据稀疏性问题</u>

具体来说，$x_h,x_i$和$x_h,x_j$的系数分别为$\langle vh,vi\rangle,\langle vh,vj\rangle$,它们之间的共同项$v_i$,因此所有包含$x_i$的非零组合特征的样本都可以用来学习隐向量$v_i$，很大程度上避免了数据稀疏性造成的影响。

* **优化**
如何利用上述公式直接求各个参数的，那么FM的复杂度为$O(kn^2)$，可以考虑如下优化

求解$\langle v_i,v_j\rangle$，主要用公式$(a+b+c)^2 - a^2 - b^2 - c^2$求得交叉项

<div align=center>

![1](img/FM0.png)
</div>

通过上述公式的变化，FM模型求解的复杂度降低为$O(kn)$

* **SGD求解各参数**

<div align=center>

![2](img/FM1.png)
</div>

* **扩展到多维**
上述FM模型只考虑了二维组合特征，我们完全可以将其扩展到多维组合特征

$$\hat y(x) = w_0 + \sum^n_{i=1}w_i x_i + \sum^d_{l=2}\sum^n_{i_l}...\sum^n_{i_l = i_{l - 1} + 1}(\prod^l_{j=1}x_{i_j})(\sum^{k_l}_{f=1}\prod^l_{j=1}v^{(l)}_{i_j,f})$$

*********************

### FFM

* **基本模型**

将特征分为不同的类别，针对不同类别的设置对应的参数

$$ \hat{y} = w_0 + \sum^{n}_{i=1}{w_{i}x_{i}} + \sum^{n}_{i = 1}\sum^n_{j=i+1}\langle v_{i,f_j}, v_{j,f_i}\rangle x_i x_j$$

$v_{i,f_j}$:表示特征$j$所属类别所对应的$v_i$

由此可以看出，FM模型就是FFM模型将所有特征都看作同一类别的特例。

### DeepFM

* **FM结构**
<div align=center>

![FM结构](\img\FM.png)

</div>

* **DNN结构**
<div align=center>

![DNN结构](\img\DNN.png)

</div>

* **DeepFM结构**
<div align=center>

![DeepFM结构](\img\DeepFM.png)

</div>

* **Deep部分**

深度部分是一个前馈神经网络，与图像或语音类的输入不同，CTR的输入一般是极其稀疏的，因此需要重新设计网络结构。在第一层隐藏层之前，引入一个嵌入层来完成输入向量压缩到低位稠密向量：

![](\img\embedding.png)

嵌入层的结构如上图所示，有两个有趣的特性：
* 尽管不同$field$的输入长度不同，但是$embedding$之后向量的长度均为k
* 在$FM$中得到的隐变量$V_{ik}$现在作为嵌入层网络的权重

嵌入层的输出为$a(0)=[e_1,e_2,...,e_m]$，其中$e_i$是嵌入的第$i$个$filed$，$m$是$field$的个数，

## Wide&Deep

## SVD系列

### SVD

### SVD++
